{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5c4833",
   "metadata": {},
   "source": [
    "# The Random Forests Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4788bec",
   "metadata": {},
   "source": [
    "Let's understand the algorithm in layman's terms. Suppose you want to go on a trip and you would like to travel to a place which you will enjoy.\n",
    "\n",
    "So what do you do to find a place that you will like? You can search online, read reviews on travel blogs and portals, or you can also ask your friends.\n",
    "\n",
    "Let's suppose you have decided to ask your friends, and talked with them about their past travel experience to various places. You will get some recommendations from every friend. Now you have to make a list of those recommended places. Then, you ask them to vote (or select one best place for the trip) from the list of recommended places you made. The place with the highest number of votes will be your final choice for the trip.\n",
    "\n",
    "In the above decision process, there are two parts. First, asking your friends about their individual travel experience and getting one recommendation out of multiple places they have visited. This part is like using the decision tree algorithm. Here, each friend makes a selection of the places he or she has visited so far.\n",
    "\n",
    "The second part, after collecting all the recommendations, is the voting procedure for selecting the best place in the list of recommendations. This whole process of getting recommendations from friends and voting on them to find the best place is known as the random forests algorithm.\n",
    "\n",
    "It technically is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result. In the case of regression, the average of all the tree outputs is considered as the final result. It is simpler and more powerful compared to the other non-linear classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1beb28d",
   "metadata": {},
   "source": [
    "# How does the Algorithm Work?\n",
    "It works in four steps:\n",
    "- Select random samples from a given dataset.\n",
    "- Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "- Perform a vote for each predicted result.\n",
    "- Select the prediction result with the most votes as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dbe1b",
   "metadata": {},
   "source": [
    "![http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4ba35",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "- Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "- It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "- The algorithm can be used in both classification and regression problems.\n",
    "- Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "- You can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
    "\n",
    "## Disadvantages:\n",
    "- Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
    "- The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acef37d",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d03dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
