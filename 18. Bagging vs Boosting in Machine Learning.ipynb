{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9223b8e",
   "metadata": {},
   "source": [
    "As we know, Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. Basic idea is to learn a set of classifiers (experts) and to allow them to vote. Bagging and Boosting are two types of Ensemble Learning. These two decrease the variance of a single estimate as they combine several estimates from different models. So the result may be a model with higher stability. Let’s understand these two terms in a glimpse.\n",
    "\n",
    "1. Bagging: It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.\n",
    "2. Boosting: It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.\n",
    "\n",
    "Let’s look at both of them in detail and understand the Difference between Bagging and Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369009c",
   "metadata": {},
   "source": [
    "![https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_2_pu8tu6.png](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_2_pu8tu6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a7d8",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "Bootstrap Aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods. Bagging is a special case of the model averaging approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4b29e",
   "metadata": {},
   "source": [
    "Suppose a set D of d tuples, at each iteration i, a training set Di of d tuples is selected via row sampling with a replacement method (i.e., there can be repetitive elements from different d tuples) from D (i.e., bootstrap). Then a classifier model Mi is learned for each training set D < i. Each classifier Mi returns its class prediction. The bagged classifier M* counts the votes and assigns the class with the most votes to X (unknown sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3761d",
   "metadata": {},
   "source": [
    "## Implementation Steps of Bagging\n",
    "\n",
    "- Step 1: Multiple subsets are created from the original data set with equal tuples, selecting observations with replacement.\n",
    "- Step 2: A base model is created on each of these subsets.\n",
    "- Step 3: Each model is learned in parallel with each training set and independent of each other.\n",
    "- Step 4: The final predictions are determined by combining the predictions from all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420a8f0",
   "metadata": {},
   "source": [
    "![https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e822538",
   "metadata": {},
   "source": [
    "**Example of Bagging**\n",
    "\n",
    "The Random Forest model uses Bagging, where decision tree models with higher variance are present. It makes random feature selection to grow trees. Several random trees make a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede20b1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30474c75",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68f66b",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "\n",
    "There are several boosting algorithms. The original ones, proposed by Robert Schapire and Yoav Freund were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize. AdaBoost was the first really successful boosting algorithm developed for the purpose of binary classification. AdaBoost is short for Adaptive Boosting and is a very popular boosting technique that combines multiple “weak classifiers” into a single “strong classifier”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672cdce7",
   "metadata": {},
   "source": [
    "## Algorithm:\n",
    "\n",
    "1. Initialise the dataset and assign equal weight to each of the data point.\n",
    "2. Provide this as input to the model and identify the wrongly classified data points.\n",
    "3. Increase the weight of the wrongly classified data points and decrease the weights of correctly classified data points. And then normalize the weights of all data points.\n",
    "4. if (got required results)\n",
    "    - Goto step 5\n",
    "    - else\n",
    "    - Goto step 2\n",
    "5. End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f9b50",
   "metadata": {},
   "source": [
    "![https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fededf",
   "metadata": {},
   "source": [
    "# Similarities Between Bagging and Boosting\n",
    "Bagging and Boosting, both being the commonly used methods, have a universal similarity of being classified as ensemble methods. Here we will explain the similarities between them.\n",
    "\n",
    "- Both are ensemble methods to get N learners from 1 learner.\n",
    "- Both generate several training data sets by random sampling.\n",
    "- Both make the final decision by averaging the N learners (or taking the majority of them i.e Majority Voting).\n",
    "- Both are good at reducing variance and provide higher stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79640e6c",
   "metadata": {},
   "source": [
    "|S.NO|Bagging|Boosting|\n",
    "|--------|--------|--------|\n",
    "| 1. | The simplest way of combining predictions that belong to the same type. | A way of combining predictions that belong to the different types. |\n",
    "| 2. | Aim to decrease variance, not bias. | Aim to decrease bias, not variance. |\n",
    "| 3. | Each model receives equal weight. | Models are weighted according to their performance. |\n",
    "| 4. | Each model is built independently. | New models are influenced | by the performance of previously built models. |\n",
    "| 5. | Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset. | Every new subset contains the elements that were misclassified by previous models. |\n",
    "| 6. | Bagging tries to solve the over-fitting problem. | Boosting tries to reduce bias. |\n",
    "| 7. | If the classifier is unstable (high variance), then apply bagging. | If the classifier is stable and simple (high bias) the apply boosting. |\n",
    "| 8. | In this base classifiers are trained parallelly. | In this base classifiers are trained sequentially. |\n",
    "| 9. | Example: The Random forest model uses Bagging. | Example: The AdaBoost uses Boosting techniques. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c873ee",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af2d10",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec724d68",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/adaboost-classifier-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d00b0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31c1a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('titanic/train.csv')\n",
    "\n",
    "data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "    \n",
    "data['Initial'].replace(\n",
    "    ['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\n",
    "\n",
    "data['Cabin'] = data['Cabin'].apply(lambda x:not isinstance(x, float))\n",
    "\n",
    "data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "label_encoder = {}\n",
    "\n",
    "for x in ['Sex', 'Embarked', 'Initial']:\n",
    "    label_encoder[x] = LabelEncoder()\n",
    "    label_encoder[x].fit(data[x])\n",
    "    data[x] = label_encoder[x].transform(data[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe5bda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[data.columns[1:]].values\n",
    "y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b25766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1848ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(random_state=42)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdf892",
   "metadata": {},
   "source": [
    "\"The most important parameters are base_estimator, n_estimators, and learning_rate.\" (Adaboost Classifier, Chris Albon)\n",
    "- base_estimator: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n",
    "- n_estimators: Number of weak learners to train iteratively.\n",
    "- learning_rate: It contributes to the weights of weak learners. It uses 1 as a default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2452cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8052434456928839\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39490178",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(random_state=42, max_iter=500,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87c2a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, base_estimator=lr, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b17b7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dc5eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8239700374531835\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2fab7",
   "metadata": {},
   "source": [
    "## Pros:\n",
    "AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results, but there is no concrete reason available.\n",
    "\n",
    "## Cons:\n",
    "AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdbe92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
