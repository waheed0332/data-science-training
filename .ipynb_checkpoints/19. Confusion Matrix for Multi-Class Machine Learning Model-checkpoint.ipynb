{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc00f79",
   "metadata": {},
   "source": [
    "For simplicity’s sake, let’s consider our multi-class classification problem to be a 3-class classification problem. Say, we have a dataset that has three class labels, namely Apple, Orange and Mango. The following is a possible confusion matrix for these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb4361",
   "metadata": {},
   "source": [
    "![https://miro.medium.com/max/720/1*yH2SM0DIUQlEiveK42NnBg.png](https://miro.medium.com/max/720/1*yH2SM0DIUQlEiveK42NnBg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648c347",
   "metadata": {},
   "source": [
    "Unlike binary classification, there are no positive or negative classes here. At first, it might be a little difficult to find TP, TN, FP and FN since there are no positive or negative classes, but it’s actually pretty easy. What we have to do here is to find TP, TN, FP and FN for each individual class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3b3460",
   "metadata": {},
   "source": [
    "For example, if we take class Apple, then let’s see what are the values of the metrics from the confusion matrix.\n",
    "\n",
    "- TP = 7\n",
    "- TN = (2+3+2+1) = 8\n",
    "- FP = (8+9) = 17\n",
    "- FN = (1+3) = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae02d2",
   "metadata": {},
   "source": [
    "Since we have all the necessary metrics for class Apple from the confusion matrix, now we can calculate the performance measures for class Apple. For example, class Apple has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4518e",
   "metadata": {},
   "source": [
    "- Precision = 7/(7+17) = 0.29\n",
    "- Recall = 7/(7+4) = 0.64\n",
    "- F1-score = 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e2bc4",
   "metadata": {},
   "source": [
    "Similarly, we can calculate the measures for the other classes. Here is a table that shows the values of each measure for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d9e76",
   "metadata": {},
   "source": [
    "![https://miro.medium.com/max/720/1*X1ghULso7P3AdMalomM6yg.png](https://miro.medium.com/max/720/1*X1ghULso7P3AdMalomM6yg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e3e22",
   "metadata": {},
   "source": [
    "Now we can do more with these measures. We can combine the F1-score of each class to have a single measure for the whole model. There are a few ways to do that, let’s look at them now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a344b",
   "metadata": {},
   "source": [
    "# Micro F1\n",
    "This is called micro-averaged F1-score. It is calculated by considering the total TP, total FP and total FN of the model. It does not consider each class individually, It calculates the metrics globally. So for our example,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7415272",
   "metadata": {},
   "source": [
    "- Total TP = (7+2+1) = 10\n",
    "- Total FP = (8+9)+(1+3)+(3+2) = 26\n",
    "- Total FN = (1+3)+(8+2)+(9+3) = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f35c0",
   "metadata": {},
   "source": [
    "Hence,\n",
    "\n",
    "- Precision = 10/(10+26) = 0.28\n",
    "- Recall = 10/(10+26) = 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ad12e",
   "metadata": {},
   "source": [
    "Now we can use the regular formula for F1-score and get the Micro F1-score using the above precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5447077",
   "metadata": {},
   "source": [
    "- Micro F1 = 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717aeca",
   "metadata": {},
   "source": [
    "# Macro F1\n",
    "This is macro-averaged F1-score. It calculates metrics for each class individually and then takes unweighted mean of the measures. As we have seen from figure “Precision, Recall and F1-score for Each Class”,\n",
    "\n",
    "- Class Apple F1-score = 0.40\n",
    "- Class Orange F1-score = 0.22\n",
    "- Class Mango F1-score = 0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3e3cc",
   "metadata": {},
   "source": [
    "Hence,\n",
    "\n",
    "- Macro F1 = (0.40+0.22+0.11)/3 = 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719108a",
   "metadata": {},
   "source": [
    "# Weighted F1\n",
    "The last one is weighted-averaged F1-score. Unlike Macro F1, it takes a weighted mean of the measures. The weights for each class are the total number of samples of that class. Since we had 11 Apples, 12 Oranges and 13 Mangoes,\n",
    "\n",
    "- Weighted F1 = ((0.40*11)+(0.22*12)+(0.11*13))/(11+12+13) = 0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2692e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[15  0  1]\n",
      " [ 0 17  4]\n",
      " [ 0  3  5]]\n",
      "\n",
      "Accuracy: 0.82\n",
      "\n",
      "Micro Precision: 0.82\n",
      "Micro Recall: 0.82\n",
      "Micro F1-score: 0.82\n",
      "\n",
      "Macro Precision: 0.78\n",
      "Macro Recall: 0.79\n",
      "Macro F1-score: 0.78\n",
      "\n",
      "Weighted Precision: 0.84\n",
      "Weighted Recall: 0.82\n",
      "Weighted F1-score: 0.83\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       1.00      0.94      0.97        16\n",
      "     Class 2       0.85      0.81      0.83        21\n",
      "     Class 3       0.50      0.62      0.56         8\n",
      "\n",
      "    accuracy                           0.82        45\n",
      "   macro avg       0.78      0.79      0.78        45\n",
      "weighted avg       0.84      0.82      0.83        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing a 3-class dataset from sklearn's toy dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3394db",
   "metadata": {},
   "source": [
    "**Note:** Scikit-Learn uses the rows to be the “true class” and the columns to be the “predicted class.” This is opposite to our consideration for the Apple, Orange and Mango example, but logically similar. You can consider true and predicted class either way. But if you are using Scikit-Learn, then you have to play by their rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c3ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
