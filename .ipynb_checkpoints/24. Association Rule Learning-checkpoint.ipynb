{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1aa5366",
   "metadata": {},
   "source": [
    "Most of you may already be familiar with clustering algorithms such as K-Means, Hierarical, or DBSCAN. However, clustering is not the only unsupervised way to find similarities between data points. You can also use association rule learning techniques to determine if certain data points (actions) are more likely to occur together.\n",
    "\n",
    "A simple example would be the supermarket shopping basket analysis. If someone is buying ground beef, does it make them more likely to also buy spaghetti? We can answer these types of questions by using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d359b",
   "metadata": {},
   "source": [
    "![https://miro.medium.com/max/720/1*yTtuCcNJrSymJfrVj28fQg.png](https://miro.medium.com/max/720/1*yTtuCcNJrSymJfrVj28fQg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e30cb",
   "metadata": {},
   "source": [
    "# What category of algorithms does Apriori belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed8036",
   "metadata": {},
   "source": [
    "Apriori is part of the association rule learning algorithms, which sit under the unsupervised branch of Machine Learning.\n",
    "\n",
    "This is because Apriori does not require us to provide a target variable for the model. Instead, the algorithm identifies relationships between data points subject to our specified constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7843eb",
   "metadata": {},
   "source": [
    "Assume we analyze the above transaction data to find frequently bought items and determine if they are often purchased together. To help us find the answers, we will make use of the following 4 metrics:\n",
    "\n",
    "- Support\n",
    "- Confidence\n",
    "- Lift\n",
    "- Conviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438f7dd",
   "metadata": {},
   "source": [
    "## Support\n",
    "The first step for us and the algorithm is to find frequently bought items. It is a straightforward calculation that is based on frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9c809",
   "metadata": {},
   "source": [
    "Support(A) = Transactions(A) / Total Transactions\n",
    "\n",
    "So in our example:\n",
    "\n",
    "- Support(Eggs) = 3/6 = 1/2 = 0.5\n",
    "- Support(Bacon) = 4/6 = 2/3 = 0.667\n",
    "- Support(Apple) = 2/6 = 1/3 = 0.333\n",
    "- ...\n",
    "- Support(Eggs&Bacon) = 3/6 = 0.5\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e040ef3",
   "metadata": {},
   "source": [
    "Here we can set our first constraint by telling the algorithm the minimum support level we want to explore, which is useful when working with large datasets. We typically want to focus computing resources to search for associations between frequently bought items while discounting the infrequent ones.\n",
    "\n",
    "For the sake of our example, letâ€™s set minimum support to 0.5, which leaves us to work with Eggs and Bacon for the rest of this example.\n",
    "\n",
    "**Important:** while Support(Eggs) and Support(Bacon) individually satisfy our minimum support constraint, it is crucial to understand that we also need the combination of them (Eggs&Bacon) to pass this constraint. Otherwise, we would not have a single item pairing to progress forward to create association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebadef3",
   "metadata": {},
   "source": [
    "## Confidence:\n",
    "Now that we have identified frequently bought items letâ€™s calculate confidence. This will tell us how confident (based on our data) we can be that an item will be purchased, given that another item has been purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83381d14",
   "metadata": {},
   "source": [
    "**Confidence(Aâ†’B)** = Probability(A & B) / Support(A)\n",
    "\n",
    "Note, confidence is the same as what is also known as conditional probability in statistics:\n",
    "\n",
    "P(B|A) = P(A & B) / P(A) \n",
    "\n",
    "Please beware of the notation. The above two equeations are equivalent, although the notations are in different order: **(Aâ†’B)** is the same as **(B|A)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979beb28",
   "metadata": {},
   "source": [
    "So, letâ€™s calculate confidence for our example:\n",
    "    \n",
    "**Confidence(Eggsâ†’Bacon)** = P(Eggs & Bacon) / Support(Eggs) = (3/6) / (3/6) = 1\n",
    "\n",
    "**Confidence(Baconâ†’Eggs)** = P(Eggs & Bacon) / Support(Bacon) = (3/6) / (2/3) = 3/4 = 0.75\n",
    "\n",
    "The above tells us that whenever eggs are bought, bacon is also bought 100% of the time. Also, whenever bacon is bought, eggs are bought 75% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3e236",
   "metadata": {},
   "source": [
    "## Lift:\n",
    "Given that different items are bought at different frequencies, how do we know that eggs and bacon really do have a strong association, and how do we measure it? You will be glad to hear that we have a way to evaluate this objectively using lift.\n",
    "\n",
    "There are multiple ways to express the formula to calculate lift. Let me first show what the formulas look like, and then I will describe an intuitive way for you to think about it.\n",
    "\n",
    "**Lift(Aâ†’B)** = Probability(A & B) / (Support(A) * Support(B))\n",
    "\n",
    "You should be able to spot that we can simplify this formula by replacing P(A&B)/Sup(A) with Confidence(Aâ†’B). Hence, we have:\n",
    "\n",
    "**Lift(Aâ†’B)** = Confidence(Aâ†’B) / Support(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18dcb9",
   "metadata": {},
   "source": [
    "Letâ€™s calculate lift for our associated items:\n",
    "\n",
    "**Lift(Eggsâ†’Bacon)** = Confidence(Eggsâ†’Bacon) / Support(Bacon) = 1 / (2/3) = 1.5\n",
    "\n",
    "**Lift(Baconâ†’Eggs)** = Confidence(Baconâ†’Eggs) / Support(Eggs) = (3/4) / (1/2) = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038fd8d",
   "metadata": {},
   "source": [
    "Lift for the two items is equal to 1.5. Note, lift>1 means that the two items are more likely to be bought together, while lift<1 means that the two items are more likely to be bought separately. Finally, lift=1 means that there is no association between the two items.\n",
    "\n",
    "An intuitive way to understand this would be to first think about the probability of eggs being bought: P(Eggs)=Support(Eggs)=0.5 as 3 out of 6 shoppers bought eggs.\n",
    "\n",
    "Then think about the probability of eggs being bought whenever bacon was bought: P(Eggs|Bacon)=Confidence(Bacon->Eggs)=0.75 since out of the 4 shoppers that bought bacon, 3 of them also bought eggs.\n",
    "\n",
    "Now, lift is simply a measure that tells us whether the probability of buying eggs increases or decreases given the purchase of bacon. Since the probability of buying eggs in such a scenario goes up from 0.5 to 0.75, we see a positive lift of 1.5 times (0.75/0.5=1.5). This means you are 1.5 times (i.e., 50%) more likely to buy eggs if you have already put bacon into your basket.\n",
    "\n",
    "See if your supermarket places these two items nearby. ðŸ˜œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce78960",
   "metadata": {},
   "source": [
    "## Conviction:\n",
    "Conviction is another way of measuring association, although it is a bit harder to get your head around. It compares the probability that A appears without B if they were independent with the actual frequency of the appearance of A without B. Letâ€™s take a look at the general formula first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c7b07",
   "metadata": {},
   "source": [
    "**Conviction(Aâ†’B)** = (1 - Support(B)) / (1 - Confidence(Aâ†’B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d109d5f",
   "metadata": {},
   "source": [
    "In our example, this would be:\n",
    "\n",
    "**Conviction(Eggsâ†’Bacon)** = (1 - Sup(Bacon) / (1 - Conf(Eggsâ†’Bacon)) = (1 - 2/3) / (1 - 1) = (1/3) / 0 = infinity\n",
    "\n",
    "**Conviction(Baconâ†’Eggs)** = (1 - Sup(Eggs) / (1 - Conf(Baconâ†’Eggs)) = (1 - 1/2) / (1 - 3/4) = (1/2) / (1/4) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a6779",
   "metadata": {},
   "source": [
    "As you can see, we had a division by 0 when calculating conviction for (Eggsâ†’Bacon) and this is because we do not have a single instance of eggs being bought without bacon (confidence=100%).\n",
    "\n",
    "In general, high confidence for Aâ†’B with low support for item B would yield a high conviction.\n",
    "\n",
    "In contrast to lift, conviction is a directed measure. Hence, while lift is the same for both (Eggsâ†’Bacon) and (Baconâ†’Eggs), conviction is different between the two, with Conv(Eggsâ†’Bacon) being much higher. Thus, you can use conviction to evaluate the directional relationship between your items.\n",
    "\n",
    "Finally, similar to lift, conviction=1 means that items are not associated, while conviction>1 indicates the relationship between the items (the higher the value, the stronger the relationship)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c00137",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af81c22",
   "metadata": {},
   "source": [
    "# Apriori algorithm:\n",
    "Apriori is a pretty straightforward algorithm that performs the following sequence of calculations:\n",
    "\n",
    "- Calculate support for itemsets of size 1.\n",
    "- Apply the minimum support threshold and prune itemsets that do not meet the threshold.\n",
    "- Move on to itemsets of size 2 and repeat steps one and two.\n",
    "- Continue the same process until no additional itemsets satisfying the minimum threshold can be found.\n",
    "- To make the process more visual, here is a diagram that illustrates what the algorithm does:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982d4ea",
   "metadata": {},
   "source": [
    "![https://miro.medium.com/max/720/1*PjV7oQE_RABg49Wsm2_Jdw.png](https://miro.medium.com/max/720/1*PjV7oQE_RABg49Wsm2_Jdw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0ba7c",
   "metadata": {},
   "source": [
    "As you can see, most itemsets in this example got pruned since they did not meet the minimum support threshold of 0.5.\n",
    "\n",
    "It is important to realize that by setting a lower minimum support threshold we would produce many more itemsets of size 2. To be precise, with a minimum support threshold of 0.3, none of the itemsets of size 1 would get pruned giving us a total of 15 itemsets of size 2 (5+4+3+2+1=15).\n",
    "\n",
    "This is not an issue when we have a small dataset, but it could become a bottleneck if you are working with a large dataset. E.g., 1,000 items can create as many as 499,500 item pairs. Hence, choose your minimum support threshold carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630c751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
